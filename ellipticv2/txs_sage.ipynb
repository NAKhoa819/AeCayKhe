{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11896965,"sourceType":"datasetVersion","datasetId":7478360}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch_geometric","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T14:03:53.665626Z","iopub.execute_input":"2025-12-02T14:03:53.665969Z","iopub.status.idle":"2025-12-02T14:03:59.035402Z","shell.execute_reply.started":"2025-12-02T14:03:53.665945Z","shell.execute_reply":"2025-12-02T14:03:59.034450Z"}},"outputs":[{"name":"stdout","text":"Collecting torch_geometric\n  Downloading torch_geometric-2.7.0-py3-none-any.whl.metadata (63 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.13.2)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2025.10.0)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.6)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (1.26.4)\nRequirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (7.1.3)\nRequirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.0.9)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.5)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.6.0)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.22.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torch_geometric) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torch_geometric) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torch_geometric) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torch_geometric) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torch_geometric) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torch_geometric) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.10.5)\nRequirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.11/dist-packages (from aiosignal>=1.4.0->aiohttp->torch_geometric) (4.15.0)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torch_geometric) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torch_geometric) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torch_geometric) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torch_geometric) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torch_geometric) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torch_geometric) (2024.2.0)\nDownloading torch_geometric-2.7.0-py3-none-any.whl (1.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: torch_geometric\nSuccessfully installed torch_geometric-2.7.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ============================================================\n# 0. Import\n# ============================================================\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import (\n    f1_score,\n    precision_recall_fscore_support,\n    classification_report,\n    confusion_matrix,\n)\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch_geometric.data import Data\nfrom torch_geometric.utils import to_undirected\nfrom torch_geometric.nn import GCN2Conv","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-02T14:03:59.036987Z","iopub.execute_input":"2025-12-02T14:03:59.037378Z","iopub.status.idle":"2025-12-02T14:04:09.476304Z","shell.execute_reply.started":"2025-12-02T14:03:59.037350Z","shell.execute_reply":"2025-12-02T14:04:09.475337Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# ============================================================\n# 1. Đọc dữ liệu Elliptic, merge, tạo label, xử lý cột\n# ============================================================\n\n# Sửa lại cho đúng path trên máy bạn\nDATA_DIR = \"/kaggle/input/elliptic/Elliptic++ Dataset\"\nTXS_FEATURES_FILE = \"txs_features.csv\"\nTXS_CLASSES_FILE  = \"txs_classes.csv\"\nTXS_EDGELIST_FILE = \"txs_edgelist.csv\"\n\ndf_feat = pd.read_csv(f\"{DATA_DIR}/{TXS_FEATURES_FILE}\")\ndf_cls  = pd.read_csv(f\"{DATA_DIR}/{TXS_CLASSES_FILE}\")\ndf_edge = pd.read_csv(f\"{DATA_DIR}/{TXS_EDGELIST_FILE}\")\n\nprint(\"txs_features shape:\", df_feat.shape)\nprint(\"txs_classes  shape:\", df_cls.shape)\nprint(\"txs_edgelist shape:\", df_edge.shape)\n\n# Merge theo txId\ndf = df_feat.merge(df_cls, on=\"txId\", how=\"left\")\n\n# Bỏ các dòng không có nhãn\ndf = df.dropna()\ndf[\"class\"] = df[\"class\"].astype(int)\n\nprint(\"\\nClass raw distribution (1=licit, 2=illicit, 3=unknown):\")\nprint(df[\"class\"].value_counts())\n\n# Bỏ class=3 (unknown)\ndf = df[df[\"class\"] != 3].copy()\n\n# Map nhãn: 1 -> 0, 2 -> 1\nlabel_map = {1: 0, 2: 1}\ndf[\"label\"] = df[\"class\"].map(label_map)\n\nprint(\"\\nLabel distribution (0=licit,1=illicit):\")\nprint(df[\"label\"].value_counts())\n\n# Đảm bảo có cột Time step\nif \"Time step\" not in df.columns:\n    raise ValueError(\"Không tìm thấy cột 'Time step' trong df!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T14:04:09.477109Z","iopub.execute_input":"2025-12-02T14:04:09.477481Z","iopub.status.idle":"2025-12-02T14:04:26.393117Z","shell.execute_reply.started":"2025-12-02T14:04:09.477463Z","shell.execute_reply":"2025-12-02T14:04:26.392443Z"}},"outputs":[{"name":"stdout","text":"txs_features shape: (203769, 184)\ntxs_classes  shape: (203769, 2)\ntxs_edgelist shape: (234355, 2)\n\nClass raw distribution (1=licit, 2=illicit, 3=unknown):\nclass\n3    156759\n2     41500\n1      4545\nName: count, dtype: int64\n\nLabel distribution (0=licit,1=illicit):\nlabel\n1    41500\n0     4545\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ============================================================\n# 2. Chọn feature: bỏ ID, nhãn, time_step\n# ============================================================\n\ncols_to_exclude = {\"txId\", \"class\", \"label\", \"Time step\"}\n\n# Chỉ lấy numeric\nnumeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\nfeature_cols = [c for c in numeric_cols if c not in cols_to_exclude]\n\nprint(\"\\nSố feature:\", len(feature_cols))\nprint(\"Một vài feature đầu:\", feature_cols[:10])\n\nX_all = df[feature_cols].astype(float).values\ny_all = df[\"label\"].values\ntxid_all = df[\"txId\"].astype(int).values\ntime_all = df[\"Time step\"].values\n\n# Chuẩn hóa feature\nscaler = StandardScaler()\nX_all_scaled = scaler.fit_transform(X_all)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T14:04:26.395031Z","iopub.execute_input":"2025-12-02T14:04:26.395284Z","iopub.status.idle":"2025-12-02T14:04:26.569366Z","shell.execute_reply.started":"2025-12-02T14:04:26.395264Z","shell.execute_reply":"2025-12-02T14:04:26.568746Z"}},"outputs":[{"name":"stdout","text":"\nSố feature: 182\nMột vài feature đầu: ['Local_feature_1', 'Local_feature_2', 'Local_feature_3', 'Local_feature_4', 'Local_feature_5', 'Local_feature_6', 'Local_feature_7', 'Local_feature_8', 'Local_feature_9', 'Local_feature_10']\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ============================================================\n# 3. Chia train / val / test theo Time step\n#    - train: Time step <= 30\n#    - val  : 31..35\n#    - test : > 35\n# (Không dùng Time step làm feature, chỉ dùng để chia tập)\n# ============================================================\n\nunique_ts = np.sort(df[\"Time step\"].unique())\nprint(\"\\nTime steps unique:\", unique_ts)\n\ntrain_ts = unique_ts[unique_ts <= 30]\nval_ts   = unique_ts[(unique_ts > 30) & (unique_ts <=39)]\ntest_ts  = unique_ts[unique_ts > 39]\n\n# Nếu vì lý do nào đó mà val/test rỗng, fallback chia tỉ lệ\nif (len(train_ts) == 0) or (len(val_ts) == 0) or (len(test_ts) == 0):\n    print(\"\\n[Cảnh báo] Tập time_step chia theo 34/41 không hợp lệ, fallback 60/20/20 theo time.\")\n    n_ts = len(unique_ts)\n    idx_train_end = int(0.6 * n_ts)\n    idx_val_end   = int(0.8 * n_ts)\n    train_ts = unique_ts[:idx_train_end]\n    val_ts   = unique_ts[idx_train_end:idx_val_end]\n    test_ts  = unique_ts[idx_val_end:]\n\nprint(\"\\nTime-step TRAIN:\", train_ts[0], \"->\", train_ts[-1])\nprint(\"Time-step VAL  :\", val_ts[0],   \"->\", val_ts[-1])\nprint(\"Time-step TEST :\", test_ts[0],  \"->\", test_ts[-1])\n\ntrain_mask = np.isin(time_all, train_ts)\nval_mask   = np.isin(time_all, val_ts)\ntest_mask  = np.isin(time_all, test_ts)\n\nX_train = X_all_scaled[train_mask]\nX_val   = X_all_scaled[val_mask]\nX_test  = X_all_scaled[test_mask]\n\ny_train = y_all[train_mask]\ny_val   = y_all[val_mask]\ny_test  = y_all[test_mask]\n\ntxid_train = txid_all[train_mask]\ntxid_val   = txid_all[val_mask]\ntxid_test  = txid_all[test_mask]\n\nprint(\"\\nKích thước:\")\nprint(\"Train:\", X_train.shape, \"Val:\", X_val.shape, \"Test:\", X_test.shape)\nprint(\"\\nPhân bố nhãn train:\")\nprint(pd.Series(y_train).value_counts())\nprint(\"\\nPhân bố nhãn val:\")\nprint(pd.Series(y_val).value_counts())\nprint(\"\\nPhân bố nhãn test:\")\nprint(pd.Series(y_test).value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T14:04:26.570085Z","iopub.execute_input":"2025-12-02T14:04:26.570269Z","iopub.status.idle":"2025-12-02T14:04:26.610361Z","shell.execute_reply.started":"2025-12-02T14:04:26.570246Z","shell.execute_reply":"2025-12-02T14:04:26.609746Z"}},"outputs":[{"name":"stdout","text":"\nTime steps unique: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n 49]\n\nTime-step TRAIN: 1 -> 30\nTime-step VAL  : 31 -> 39\nTime-step TEST : 40 -> 49\n\nKích thước:\nTrain: (26750, 182) Val: (8221, 182) Test: (11074, 182)\n\nPhân bố nhãn train:\n1    23796\n0     2954\nName: count, dtype: int64\n\nPhân bố nhãn val:\n1    7266\n0     955\nName: count, dtype: int64\n\nPhân bố nhãn test:\n1    10438\n0      636\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ============================================================\n# 4. Build edge_index cho từng subset (train/val/test)\n#    Dùng txs_edgelist, chuyển sang graph vô hướng\n# ============================================================\n\ndef build_edge_index(df_edges, valid_txids):\n    \"\"\"\n    df_edges: DataFrame với cột txId1, txId2 (ID node gốc trong toàn graph)\n    valid_txids: mảng txId thuộc tập node (train/val/test)\n    Trả:\n      edge_index: LongTensor [2, num_edges] với index nội bộ [0..num_nodes-1]\n    \"\"\"\n    node_ids = np.asarray(valid_txids, dtype=np.int64)\n    id2idx = {tid: i for i, tid in enumerate(node_ids)}\n\n    mask = df_edges[\"txId1\"].isin(node_ids) & df_edges[\"txId2\"].isin(node_ids)\n    edges_sub = df_edges.loc[mask, [\"txId1\", \"txId2\"]]\n\n    if len(edges_sub) == 0:\n        # graph rỗng, tạo edge_index size (2, 0)\n        edges_idx = np.zeros((2, 0), dtype=np.int64)\n        edge_index = torch.tensor(edges_idx, dtype=torch.long)\n        return edge_index\n\n    src_idx = edges_sub[\"txId1\"].map(id2idx).values\n    dst_idx = edges_sub[\"txId2\"].map(id2idx).values\n    edges_idx = np.vstack([src_idx, dst_idx])\n\n    edge_index = torch.tensor(edges_idx, dtype=torch.long)\n    edge_index = to_undirected(edge_index)\n    return edge_index\n\nedge_index_train = build_edge_index(df_edge, txid_train)\nedge_index_val   = build_edge_index(df_edge, txid_val)\nedge_index_test  = build_edge_index(df_edge, txid_test)\n\nprint(\"\\nTrain edges:\", edge_index_train.size(1),\n      \"Val edges:\", edge_index_val.size(1),\n      \"Test edges:\", edge_index_test.size(1))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T14:04:26.611032Z","iopub.execute_input":"2025-12-02T14:04:26.611225Z","iopub.status.idle":"2025-12-02T14:04:26.877711Z","shell.execute_reply.started":"2025-12-02T14:04:26.611209Z","shell.execute_reply":"2025-12-02T14:04:26.876975Z"}},"outputs":[{"name":"stdout","text":"\nTrain edges: 40860 Val edges: 13172 Test edges: 18792\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# ============================================================\n# 5. Tạo PyG Data object\n# ============================================================\n\nX_train_gcn = torch.tensor(X_train, dtype=torch.float)\nX_val_gcn   = torch.tensor(X_val,   dtype=torch.float)\nX_test_gcn  = torch.tensor(X_test,  dtype=torch.float)\n\ny_train_gcn = torch.tensor(y_train, dtype=torch.long)\ny_val_gcn   = torch.tensor(y_val,   dtype=torch.long)\ny_test_gcn  = torch.tensor(y_test,  dtype=torch.long)\n\ntrain_data = Data(x=X_train_gcn, edge_index=edge_index_train, y=y_train_gcn)\nval_data   = Data(x=X_val_gcn,   edge_index=edge_index_val,   y=y_val_gcn)\ntest_data  = Data(x=X_test_gcn,  edge_index=edge_index_test,  y=y_test_gcn)\n\n# Lưu txId (cho debug nếu cần)\ntrain_data.node_ids = torch.tensor(txid_train, dtype=torch.long)\nval_data.node_ids   = torch.tensor(txid_val,   dtype=torch.long)\ntest_data.node_ids  = torch.tensor(txid_test,  dtype=torch.long)\n\nprint(\"\\ntrain_data:\", train_data)\nprint(\"val_data  :\", val_data)\nprint(\"test_data :\", test_data)\n\nprint(\"\\nCheck NaN in train features:\", torch.isnan(train_data.x).any().item())\nprint(\"Check Inf in train features:\", torch.isinf(train_data.x).any().item())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T14:04:26.878582Z","iopub.execute_input":"2025-12-02T14:04:26.878925Z","iopub.status.idle":"2025-12-02T14:04:26.907246Z","shell.execute_reply.started":"2025-12-02T14:04:26.878907Z","shell.execute_reply":"2025-12-02T14:04:26.906535Z"}},"outputs":[{"name":"stdout","text":"\ntrain_data: Data(x=[26750, 182], edge_index=[2, 40860], y=[26750], node_ids=[26750])\nval_data  : Data(x=[8221, 182], edge_index=[2, 13172], y=[8221], node_ids=[8221])\ntest_data : Data(x=[11074, 182], edge_index=[2, 18792], y=[11074], node_ids=[11074])\n\nCheck NaN in train features: False\nCheck Inf in train features: False\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# ============================================================\n# 6. Device + class weights (giống blte_gcn)\n# ============================================================\n\nif hasattr(torch, \"xpu\") and torch.xpu.is_available():\n    device = torch.device(\"xpu\")\nelif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cpu\")\n\nprint(\"\\nDevice:\", device)\n\nclass_sample_count = torch.bincount(train_data.y, minlength=2).float()\neps = 1e-8\ninv_freq = 1.0 / (class_sample_count + eps)\nnorm_inv_freq = inv_freq / inv_freq.min()\n\nprint(\"Class counts (train):\", class_sample_count.tolist())\nprint(\"Class weights (inv_freq normalized):\", norm_inv_freq.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T14:04:26.908021Z","iopub.execute_input":"2025-12-02T14:04:26.908284Z","iopub.status.idle":"2025-12-02T14:04:26.914883Z","shell.execute_reply.started":"2025-12-02T14:04:26.908258Z","shell.execute_reply":"2025-12-02T14:04:26.914145Z"}},"outputs":[{"name":"stdout","text":"\nDevice: cuda\nClass counts (train): [2954.0, 23796.0]\nClass weights (inv_freq normalized): [8.05551815032959, 1.0]\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"from torch_geometric.nn import SAGEConv\nclass GraphSAGE(nn.Module):\n    def __init__(self, in_dim, hid_dim, out_dim, num_layers=2, dropout=0.5):\n        super().__init__()\n        self.dropout = dropout\n\n        convs = []\n        # layer đầu: in_dim -> hid_dim\n        convs.append(SAGEConv(in_dim, hid_dim))\n        # các layer hidden: hid_dim -> hid_dim\n        for _ in range(num_layers - 1):\n            convs.append(SAGEConv(hid_dim, hid_dim))\n        self.convs = nn.ModuleList(convs)\n\n        self.lin_out = nn.Linear(hid_dim, out_dim)\n\n    def forward(self, x, edge_index):\n        for conv in self.convs:\n            x = conv(x, edge_index)\n            x = F.relu(x)\n            x = F.dropout(x, p=self.dropout, training=self.training)\n        out = self.lin_out(x)\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T14:04:26.915673Z","iopub.execute_input":"2025-12-02T14:04:26.916185Z","iopub.status.idle":"2025-12-02T14:04:26.930261Z","shell.execute_reply.started":"2025-12-02T14:04:26.916163Z","shell.execute_reply":"2025-12-02T14:04:26.929578Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"\n\nclass FocalLoss(nn.Module):\n    def __init__(self, gamma=2.0, weight=None, reduction=\"mean\"):\n        super().__init__()\n        self.gamma = gamma\n        self.weight = weight\n        self.reduction = reduction\n\n    def forward(self, logits, target):\n        logp = F.log_softmax(logits, dim=1)\n        p = logp.exp()\n\n        target = target.view(-1, 1)\n        logp_t = logp.gather(1, target).squeeze(1)\n        p_t    = p.gather(1, target).squeeze(1)\n\n        focal_term = (1 - p_t) ** self.gamma\n        loss = - focal_term * logp_t\n\n        if self.weight is not None:\n            w = self.weight[target.squeeze(1)].view(-1)\n            loss = loss * w\n\n        if self.reduction == \"mean\":\n            return loss.mean()\n        elif self.reduction == \"sum\":\n            return loss.sum()\n        else:\n            return loss\n\n\ndef train_one_config(config, loss_type=\"ce\", class_weights=None,\n                     max_epochs=400, patience=30, verbose=False):\n    in_dim  = train_data.x.size(1)\n    out_dim = 2\n\n    model = GraphSAGE(\n        in_dim=in_dim,\n        hid_dim=config[\"hid_dim\"],\n        out_dim=out_dim,\n        num_layers=config[\"num_layers\"],\n        dropout=config.get(\"dropout\", 0.5),\n    ).to(device)\n\n    cw = class_weights.to(device) if class_weights is not None else None\n\n    if loss_type == \"ce\":\n        crit = nn.CrossEntropyLoss(weight=cw)\n    elif loss_type == \"focal\":\n        crit = FocalLoss(gamma=config.get(\"gamma\", 2.0), weight=cw)\n    else:\n        raise ValueError(\"loss_type must be 'ce' or 'focal'\")\n\n    opt = torch.optim.Adam(\n        model.parameters(),\n        lr=config[\"lr\"],\n        weight_decay=config[\"weight_decay\"],\n    )\n\n    def eval_for_search(data):\n        model.eval()\n        with torch.no_grad():\n            out = model(data.x.to(device), data.edge_index.to(device))\n            loss = crit(out, data.y.to(device)).item()\n            preds = out.argmax(dim=1).cpu().numpy()\n            y_true = data.y.cpu().numpy()\n            macro_f1 = f1_score(y_true, preds, average=\"macro\", zero_division=0)\n        return loss, macro_f1\n\n    best_state = None\n    best_val_macro = -1.0\n    patience_counter = 0\n\n    for epoch in range(1, max_epochs + 1):\n        model.train()\n        opt.zero_grad()\n        out = model(train_data.x.to(device), train_data.edge_index.to(device))\n        loss_train = crit(out, train_data.y.to(device))\n        loss_train.backward()\n        opt.step()\n\n        val_loss, val_macro = eval_for_search(val_data)\n\n        if verbose and (epoch % 20 == 0 or epoch == 1):\n            print(f\"[{config.get('name','?')}] Epoch {epoch:03d} \"\n                  f\"- train_loss={loss_train.item():.4f} \"\n                  f\"- val_loss={val_loss:.4f} \"\n                  f\"- val_macro={val_macro:.4f}\")\n\n        if val_macro > best_val_macro + 1e-4:\n            best_val_macro = val_macro\n            best_state = torch.save(model.state_dict(), \"tmp_best_tx_model.pt\")\n            best_state = torch.load(\"tmp_best_tx_model.pt\", map_location=\"cpu\")\n            patience_counter = 0\n        else:\n            patience_counter += 1\n\n        if patience_counter >= patience:\n            if verbose:\n                print(f\"Early stop (no improve {patience} epochs)\")\n            break\n\n    if best_state is not None:\n        model.load_state_dict(best_state)\n\n    return model, best_val_macro","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T14:04:26.931931Z","iopub.execute_input":"2025-12-02T14:04:26.932119Z","iopub.status.idle":"2025-12-02T14:04:26.948627Z","shell.execute_reply.started":"2025-12-02T14:04:26.932104Z","shell.execute_reply":"2025-12-02T14:04:26.948088Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# ============================================================\n# 8. Hàm tune threshold + evaluate_with_threshold\n# ============================================================\n\n@torch.no_grad()\ndef tune_threshold_on_val(model, data, thresholds=None):\n    model.eval()\n    if thresholds is None:\n        thresholds = np.linspace(0.1, 0.9, 17)\n\n    out = model(data.x.to(device), data.edge_index.to(device))\n    probs = F.softmax(out, dim=1)[:, 1].cpu().numpy()  # xác suất class=1 (illicit)\n    y_true = data.y.cpu().numpy()\n\n    results = []\n    for t in thresholds:\n        preds = (probs >= t).astype(int)\n        macro_f1 = f1_score(y_true, preds, average=\"macro\", zero_division=0)\n        results.append((t, macro_f1))\n\n    best_t, best_macro = max(results, key=lambda x: x[1])\n    print(\"\\nThreshold search (val):\")\n    for t, m in results:\n        print(f\"  t={t:.2f}  macro-F1={m:.4f}\")\n    print(f\"\\nBest threshold on VAL: t={best_t:.2f}, macro-F1={best_macro:.4f}\")\n    return best_t, best_macro\n\n\n@torch.no_grad()\ndef evaluate_with_threshold(model, data, threshold=0.5, name=\"SET\"):\n    model.eval()\n    out = model(data.x.to(device), data.edge_index.to(device))\n    probs = F.softmax(out, dim=1)[:, 1].cpu().numpy()\n    y_true = data.y.cpu().numpy()\n    preds = (probs >= threshold).astype(int)\n\n    f1_scam  = f1_score(y_true, preds, pos_label=1, zero_division=0)\n    micro_f1 = f1_score(y_true, preds, average=\"micro\", zero_division=0)\n    macro_f1 = f1_score(y_true, preds, average=\"macro\", zero_division=0)\n    cm = confusion_matrix(y_true, preds, labels=[0, 1])\n\n    print(f\"\\n{name} with threshold={threshold:.2f}\")\n    print(\"F1 (illicit=1):\", f1_scam)\n    print(\"Micro-F1:\", micro_f1)\n    print(\"Macro-F1:\", macro_f1)\n    print(\"Confusion matrix:\\n\", cm)\n    print(\"\\nClassification report:\")\n    print(classification_report(y_true, preds, digits=4))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T14:04:26.949318Z","iopub.execute_input":"2025-12-02T14:04:26.949536Z","iopub.status.idle":"2025-12-02T14:04:26.968829Z","shell.execute_reply.started":"2025-12-02T14:04:26.949517Z","shell.execute_reply":"2025-12-02T14:04:26.968254Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# ============================================================\n# 9. Grid search arch × loss, chọn best model theo macro-F1 VAL\n# ============================================================\n\narch_space = [\n    {\"name\": \"arch1\", \"hid_dim\": 64,  \"num_layers\": 8,  \"dropout\": 0.5,\n     \"lr\": 1e-2, \"weight_decay\": 5e-4, \"alpha\": 0.1},\n    {\"name\": \"arch2\", \"hid_dim\": 64,  \"num_layers\": 16, \"dropout\": 0.5,\n     \"lr\": 1e-2, \"weight_decay\": 5e-4, \"alpha\": 0.1},\n    {\"name\": \"arch3\", \"hid_dim\": 128, \"num_layers\": 16, \"dropout\": 0.5,\n     \"lr\": 5e-3, \"weight_decay\": 5e-4, \"alpha\": 0.1},\n    {\"name\": \"arch4\", \"hid_dim\": 128, \"num_layers\": 32, \"dropout\": 0.5,\n     \"lr\": 5e-3, \"weight_decay\": 1e-3,\"alpha\": 0.1},\n]\n\nloss_configs = [\n    {\"name\": \"CE_no_weight\",   \"loss_type\": \"ce\",\n     \"class_weights\": torch.tensor([1.0, 1.0])},\n    {\"name\": \"CE_inv_freq\",    \"loss_type\": \"ce\",\n     \"class_weights\": norm_inv_freq},\n    {\"name\": \"Focal_gamma1.5\", \"loss_type\": \"focal\",\n     \"class_weights\": norm_inv_freq, \"gamma\": 1.5},\n    {\"name\": \"Focal_gamma2.0\", \"loss_type\": \"focal\",\n     \"class_weights\": norm_inv_freq, \"gamma\": 2.0},\n]\n\nbest_model = None\nbest_arch  = None\nbest_loss_cfg = None\nbest_val_macro = -1.0\n\nfor arch in arch_space:\n    for lc in loss_configs:\n        cfg = arch.copy()\n        cfg[\"name\"] = arch[\"name\"] + \"_\" + lc[\"name\"]\n        if lc[\"loss_type\"] == \"focal\":\n            cfg[\"gamma\"] = lc[\"gamma\"]\n\n        print(f\"\\n=== Training config: {cfg['name']} (loss={lc['loss_type']}) ===\")\n        model_cfg, val_macro = train_one_config(\n            cfg,\n            loss_type=lc[\"loss_type\"],\n            class_weights=lc[\"class_weights\"],\n            max_epochs=300,\n            patience=40,\n            verbose=False,\n        )\n        print(f\"Config {cfg['name']}  val_macro = {val_macro:.4f}\")\n\n        if val_macro > best_val_macro:\n            best_val_macro = val_macro\n            best_model = model_cfg\n            best_arch = arch\n            best_loss_cfg = lc\n\nprint(\"\\n>>> BEST CONFIG OVERALL\")\nprint(\"Best arch:\", best_arch)\nprint(\"Best loss config:\", best_loss_cfg)\nprint(\"Best val macro-F1:\", best_val_macro)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T14:04:26.969549Z","iopub.execute_input":"2025-12-02T14:04:26.969816Z","iopub.status.idle":"2025-12-02T14:05:16.743033Z","shell.execute_reply.started":"2025-12-02T14:04:26.969791Z","shell.execute_reply":"2025-12-02T14:05:16.742328Z"}},"outputs":[{"name":"stdout","text":"\n=== Training config: arch1_CE_no_weight (loss=ce) ===\nConfig arch1_CE_no_weight  val_macro = 0.4692\n\n=== Training config: arch1_CE_inv_freq (loss=ce) ===\nConfig arch1_CE_inv_freq  val_macro = 0.8552\n\n=== Training config: arch1_Focal_gamma1.5 (loss=focal) ===\nConfig arch1_Focal_gamma1.5  val_macro = 0.8601\n\n=== Training config: arch1_Focal_gamma2.0 (loss=focal) ===\nConfig arch1_Focal_gamma2.0  val_macro = 0.8578\n\n=== Training config: arch2_CE_no_weight (loss=ce) ===\nConfig arch2_CE_no_weight  val_macro = 0.4692\n\n=== Training config: arch2_CE_inv_freq (loss=ce) ===\nConfig arch2_CE_inv_freq  val_macro = 0.5770\n\n=== Training config: arch2_Focal_gamma1.5 (loss=focal) ===\nConfig arch2_Focal_gamma1.5  val_macro = 0.5770\n\n=== Training config: arch2_Focal_gamma2.0 (loss=focal) ===\nConfig arch2_Focal_gamma2.0  val_macro = 0.5770\n\n=== Training config: arch3_CE_no_weight (loss=ce) ===\nConfig arch3_CE_no_weight  val_macro = 0.4692\n\n=== Training config: arch3_CE_inv_freq (loss=ce) ===\nConfig arch3_CE_inv_freq  val_macro = 0.5770\n\n=== Training config: arch3_Focal_gamma1.5 (loss=focal) ===\nConfig arch3_Focal_gamma1.5  val_macro = 0.5770\n\n=== Training config: arch3_Focal_gamma2.0 (loss=focal) ===\nConfig arch3_Focal_gamma2.0  val_macro = 0.5770\n\n=== Training config: arch4_CE_no_weight (loss=ce) ===\nConfig arch4_CE_no_weight  val_macro = 0.4692\n\n=== Training config: arch4_CE_inv_freq (loss=ce) ===\nConfig arch4_CE_inv_freq  val_macro = 0.5770\n\n=== Training config: arch4_Focal_gamma1.5 (loss=focal) ===\nConfig arch4_Focal_gamma1.5  val_macro = 0.5770\n\n=== Training config: arch4_Focal_gamma2.0 (loss=focal) ===\nConfig arch4_Focal_gamma2.0  val_macro = 0.5770\n\n>>> BEST CONFIG OVERALL\nBest arch: {'name': 'arch1', 'hid_dim': 64, 'num_layers': 8, 'dropout': 0.5, 'lr': 0.01, 'weight_decay': 0.0005, 'alpha': 0.1}\nBest loss config: {'name': 'Focal_gamma1.5', 'loss_type': 'focal', 'class_weights': tensor([8.0555, 1.0000]), 'gamma': 1.5}\nBest val macro-F1: 0.8601463162068218\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# ============================================================\n# 10. Tuning threshold & Evaluate trên train/val/test\n# ============================================================\n\nbest_threshold, _ = tune_threshold_on_val(best_model, val_data)\n\nevaluate_with_threshold(best_model, train_data, best_threshold, name=\"TRAIN\")\nevaluate_with_threshold(best_model, val_data,   best_threshold, name=\"VAL\")\nevaluate_with_threshold(best_model, test_data,  best_threshold, name=\"TEST\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T14:05:16.744236Z","iopub.execute_input":"2025-12-02T14:05:16.744790Z","iopub.status.idle":"2025-12-02T14:05:16.856410Z","shell.execute_reply.started":"2025-12-02T14:05:16.744770Z","shell.execute_reply":"2025-12-02T14:05:16.855745Z"}},"outputs":[{"name":"stdout","text":"\nThreshold search (val):\n  t=0.10  macro-F1=0.7339\n  t=0.15  macro-F1=0.8473\n  t=0.20  macro-F1=0.8641\n  t=0.25  macro-F1=0.8617\n  t=0.30  macro-F1=0.8630\n  t=0.35  macro-F1=0.8649\n  t=0.40  macro-F1=0.8672\n  t=0.45  macro-F1=0.8654\n  t=0.50  macro-F1=0.8601\n  t=0.55  macro-F1=0.8452\n  t=0.60  macro-F1=0.8142\n  t=0.65  macro-F1=0.7862\n  t=0.70  macro-F1=0.7524\n  t=0.75  macro-F1=0.7216\n  t=0.80  macro-F1=0.6893\n  t=0.85  macro-F1=0.6541\n  t=0.90  macro-F1=0.6178\n\nBest threshold on VAL: t=0.40, macro-F1=0.8672\n\nTRAIN with threshold=0.40\nF1 (illicit=1): 0.995940004627974\nMicro-F1: 0.9927850467289719\nMacro-F1: 0.9817868730166535\nConfusion matrix:\n [[ 2885    69]\n [  124 23672]]\n\nClassification report:\n              precision    recall  f1-score   support\n\n           0     0.9588    0.9766    0.9676      2954\n           1     0.9971    0.9948    0.9959     23796\n\n    accuracy                         0.9928     26750\n   macro avg     0.9779    0.9857    0.9818     26750\nweighted avg     0.9929    0.9928    0.9928     26750\n\n\nVAL with threshold=0.40\nF1 (illicit=1): 0.9660366831717694\nMicro-F1: 0.9407614645420265\nMacro-F1: 0.8672313705920663\nConfusion matrix:\n [[ 808  147]\n [ 340 6926]]\n\nClassification report:\n              precision    recall  f1-score   support\n\n           0     0.7038    0.8461    0.7684       955\n           1     0.9792    0.9532    0.9660      7266\n\n    accuracy                         0.9408      8221\n   macro avg     0.8415    0.8996    0.8672      8221\nweighted avg     0.9472    0.9408    0.9431      8221\n\n\nTEST with threshold=0.40\nF1 (illicit=1): 0.970071349901834\nMicro-F1: 0.9435614953946181\nMacro-F1: 0.7380001018283873\nConfusion matrix:\n [[  320   316]\n [  309 10129]]\n\nClassification report:\n              precision    recall  f1-score   support\n\n           0     0.5087    0.5031    0.5059       636\n           1     0.9697    0.9704    0.9701     10438\n\n    accuracy                         0.9436     11074\n   macro avg     0.7392    0.7368    0.7380     11074\nweighted avg     0.9433    0.9436    0.9434     11074\n\n","output_type":"stream"}],"execution_count":13}]}