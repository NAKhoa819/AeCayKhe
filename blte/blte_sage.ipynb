{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0c4366f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\elliptic\\venv1\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 0. Import\n",
    "# ============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    precision_recall_fscore_support,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    ")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_undirected\n",
    "from torch_geometric.nn import GCN2Conv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81566b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw shape: (71250, 18)\n",
      "Label distribution:\n",
      " label\n",
      "0    57000\n",
      "1    14250\n",
      "Name: count, dtype: int64\n",
      "Columns after dropping time cols: ['hash', 'nonce', 'transaction_index', 'from_address', 'to_address', 'value', 'gas', 'gas_price', 'input', 'receipt_cumulative_gas_used', 'receipt_gas_used', 'block_hash', 'from_scam', 'to_scam', 'from_category', 'to_category', 'label', 'txId']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 1. Đọc dữ liệu BLTE, tạo label, xử lý cột\n",
    "# ============================================================\n",
    "\n",
    "# Đổi path theo file của bạn\n",
    "DATA_PATH = r\"D:\\elliptic\\blte\\Labeled-Transactions-based-Dataset-of-Ethereum-Network-master\\FinalDataset.xlsx\"\n",
    "df = pd.read_excel(DATA_PATH)\n",
    "\n",
    "print(\"Raw shape:\", df.shape)\n",
    "\n",
    "# Tạo label từ from_scam / to_scam\n",
    "df[\"from_scam\"] = df[\"from_scam\"].fillna(0).astype(int)\n",
    "df[\"to_scam\"]   = df[\"to_scam\"].fillna(0).astype(int)\n",
    "\n",
    "df[\"label\"] = ((df[\"from_scam\"] == 1) | (df[\"to_scam\"] == 1)).astype(int)\n",
    "print(\"Label distribution:\\n\", df[\"label\"].value_counts())\n",
    "\n",
    "# Reset index, gán txId = index (node id)\n",
    "df = df.reset_index(drop=True)\n",
    "df[\"txId\"] = np.arange(len(df), dtype=int)\n",
    "\n",
    "# BỎ HẲN 2 cột thời gian khỏi data từ đầu\n",
    "for col in [\"block_timestamp\", \"block_number\"]:\n",
    "    if col in df.columns:\n",
    "        df.drop(columns=[col], inplace=True)\n",
    "\n",
    "print(\"Columns after dropping time cols:\", df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e635dc28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature columns: ['nonce', 'transaction_index', 'value', 'gas', 'gas_price', 'receipt_cumulative_gas_used', 'receipt_gas_used']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 2. Chọn feature: bỏ ID, địa chỉ, nhãn, scam flag, category\n",
    "# ============================================================\n",
    "\n",
    "id_and_addr_cols = [\n",
    "    \"hash\",\n",
    "    \"from_address\",\n",
    "    \"to_address\",\n",
    "    \"block_hash\",\n",
    "    \"input\",\n",
    "    \"txId\",\n",
    "]\n",
    "\n",
    "label_related_cols = [\n",
    "    \"from_scam\",\n",
    "    \"to_scam\",\n",
    "    \"from_category\",\n",
    "    \"to_category\",\n",
    "    \"label\",\n",
    "]\n",
    "\n",
    "cols_to_exclude = set(id_and_addr_cols + label_related_cols)\n",
    "\n",
    "feature_cols = [c for c in df.columns if c not in cols_to_exclude]\n",
    "print(\"Feature columns:\", feature_cols)\n",
    "\n",
    "X_all = df[feature_cols].astype(float).values\n",
    "y_all = df[\"label\"].values\n",
    "txid_all = df[\"txId\"].values\n",
    "\n",
    "# Chuẩn hóa feature\n",
    "scaler = StandardScaler()\n",
    "X_all_scaled = scaler.fit_transform(X_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33a4b16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num raw edges: 67812\n",
      "Num unique edges: 65764\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 3. Build transaction graph KHÔNG dùng thời gian\n",
    "#    - Với mỗi địa chỉ, nối các tx có cùng địa chỉ theo thứ tự xuất hiện\n",
    "# ============================================================\n",
    "\n",
    "def build_tx_edges(df, addr_col, txid_col=\"txId\"):\n",
    "    \"\"\"\n",
    "    Với mỗi địa chỉ (from_address / to_address):\n",
    "      - lấy list txId theo thứ tự trong DataFrame\n",
    "      - nối tx liên tiếp thành cạnh (tx[i], tx[i+1])\n",
    "    KHÔNG dùng block_timestamp.\n",
    "    \"\"\"\n",
    "    edges = []\n",
    "    for addr, group in df.groupby(addr_col):\n",
    "        if len(group) <= 1:\n",
    "            continue\n",
    "        ids = group[txid_col].values\n",
    "        src = ids[:-1]\n",
    "        dst = ids[1:]\n",
    "        edges.extend(zip(src, dst))\n",
    "    return edges\n",
    "\n",
    "edges_from = build_tx_edges(df, \"from_address\")\n",
    "edges_to   = build_tx_edges(df, \"to_address\")\n",
    "\n",
    "all_edges = edges_from + edges_to\n",
    "print(\"Num raw edges:\", len(all_edges))\n",
    "\n",
    "if len(all_edges) > 0:\n",
    "    edges_array = np.array(all_edges, dtype=np.int64)\n",
    "    # Đưa về dạng undirected unique\n",
    "    edges_array = np.sort(edges_array, axis=1)\n",
    "    edges_array = np.unique(edges_array, axis=0)\n",
    "else:\n",
    "    edges_array = np.empty((0, 2), dtype=np.int64)\n",
    "\n",
    "print(\"Num unique edges:\", len(edges_array))\n",
    "\n",
    "df_edges = pd.DataFrame(edges_array, columns=[\"txId1\", \"txId2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7de39e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 49874 Val size: 10688 Test size: 10688\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 4. Split train / val / test NGẪU NHIÊN, 70 / 15 / 15 với stratify\n",
    "# ============================================================\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE  = 0.15\n",
    "VAL_SIZE   = 0.15\n",
    "TRAIN_SIZE = 0.70\n",
    "\n",
    "# B1: tách TEST trước\n",
    "X_temp, X_test, y_temp, y_test, txid_temp, txid_test = train_test_split(\n",
    "    X_all_scaled, y_all, txid_all,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y_all\n",
    ")\n",
    "\n",
    "# B2: tách TRAIN và VAL từ phần còn lại\n",
    "#   tỉ lệ val trên (train+val)\n",
    "val_ratio_in_temp = VAL_SIZE / (TRAIN_SIZE + VAL_SIZE)  # 0.15 / 0.85\n",
    "\n",
    "X_train, X_val, y_train, y_val, txid_train, txid_val = train_test_split(\n",
    "    X_temp, y_temp, txid_temp,\n",
    "    test_size=val_ratio_in_temp,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y_temp\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(X_train),\n",
    "      \"Val size:\", len(X_val),\n",
    "      \"Test size:\", len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "280afe72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train edges: 64410 Val edges: 2892 Test edges: 2990\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 5. Build edge_index cho từng subset (train/val/test)\n",
    "# ============================================================\n",
    "\n",
    "def build_edge_index(df_edges, valid_txids):\n",
    "    \"\"\"\n",
    "    df_edges: DataFrame với cột txId1, txId2 (ID node gốc)\n",
    "    valid_txids: mảng txId thuộc tập node (train/val/test)\n",
    "    Trả:\n",
    "      edge_index: LongTensor [2, num_edges] với index nội bộ [0..num_nodes-1]\n",
    "    \"\"\"\n",
    "    node_ids = np.asarray(valid_txids, dtype=np.int64)\n",
    "    id2idx = {tid: i for i, tid in enumerate(node_ids)}\n",
    "\n",
    "    mask = df_edges[\"txId1\"].isin(node_ids) & df_edges[\"txId2\"].isin(node_ids)\n",
    "    edges_sub = df_edges.loc[mask, [\"txId1\", \"txId2\"]]\n",
    "\n",
    "    if len(edges_sub) == 0:\n",
    "        return torch.empty((2, 0), dtype=torch.long)\n",
    "\n",
    "    src_idx = edges_sub[\"txId1\"].map(id2idx).values\n",
    "    dst_idx = edges_sub[\"txId2\"].map(id2idx).values\n",
    "    edges_idx = np.vstack([src_idx, dst_idx])\n",
    "\n",
    "    edge_index = torch.tensor(edges_idx, dtype=torch.long)\n",
    "    edge_index = to_undirected(edge_index)\n",
    "    return edge_index\n",
    "\n",
    "edge_index_train = build_edge_index(df_edges, txid_train)\n",
    "edge_index_val   = build_edge_index(df_edges, txid_val)\n",
    "edge_index_test  = build_edge_index(df_edges, txid_test)\n",
    "\n",
    "print(\"Train edges:\", edge_index_train.size(1),\n",
    "      \"Val edges:\", edge_index_val.size(1),\n",
    "      \"Test edges:\", edge_index_test.size(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bc87276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[49874, 7], edge_index=[2, 64410], y=[49874], node_ids=[49874])\n",
      "Data(x=[10688, 7], edge_index=[2, 2892], y=[10688], node_ids=[10688])\n",
      "Data(x=[10688, 7], edge_index=[2, 2990], y=[10688], node_ids=[10688])\n",
      "Check NaN in train features: False\n",
      "Check Inf in train features: False\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 6. Tạo PyG Data object\n",
    "# ============================================================\n",
    "\n",
    "X_train_gcn = torch.tensor(X_train, dtype=torch.float)\n",
    "X_val_gcn   = torch.tensor(X_val,   dtype=torch.float)\n",
    "X_test_gcn  = torch.tensor(X_test,  dtype=torch.float)\n",
    "\n",
    "y_train_gcn = torch.tensor(y_train, dtype=torch.long)\n",
    "y_val_gcn   = torch.tensor(y_val,   dtype=torch.long)\n",
    "y_test_gcn  = torch.tensor(y_test,  dtype=torch.long)\n",
    "\n",
    "train_data = Data(x=X_train_gcn, edge_index=edge_index_train, y=y_train_gcn)\n",
    "val_data   = Data(x=X_val_gcn,   edge_index=edge_index_val,   y=y_val_gcn)\n",
    "test_data  = Data(x=X_test_gcn,  edge_index=edge_index_test,  y=y_test_gcn)\n",
    "\n",
    "# Lưu txId (cho debug nếu cần)\n",
    "train_data.node_ids = torch.tensor(txid_train, dtype=torch.long)\n",
    "val_data.node_ids   = torch.tensor(txid_val,   dtype=torch.long)\n",
    "test_data.node_ids  = torch.tensor(txid_test,  dtype=torch.long)\n",
    "\n",
    "print(train_data)\n",
    "print(val_data)\n",
    "print(test_data)\n",
    "\n",
    "print(\"Check NaN in train features:\", torch.isnan(train_data.x).any().item())\n",
    "print(\"Check Inf in train features:\", torch.isinf(train_data.x).any().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdc9c4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: xpu\n",
      "Class counts (train): [39900.0, 9974.0]\n",
      "Class weights (inv_freq normalized): [1.0, 4.000401020050049]\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 7. Device + class weights\n",
    "# ============================================================\n",
    "\n",
    "if hasattr(torch, \"xpu\") and torch.xpu.is_available():\n",
    "    device = torch.device(\"xpu\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Device:\", device)\n",
    "\n",
    "class_sample_count = torch.bincount(train_data.y, minlength=2).float()\n",
    "eps = 1e-8\n",
    "inv_freq = 1.0 / (class_sample_count + eps)\n",
    "norm_inv_freq = inv_freq / inv_freq.min()\n",
    "\n",
    "print(\"Class counts (train):\", class_sample_count.tolist())\n",
    "print(\"Class weights (inv_freq normalized):\", norm_inv_freq.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae65efa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import SAGEConv\n",
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim, out_dim, num_layers=2, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "\n",
    "        convs = []\n",
    "        # layer đầu: in_dim -> hid_dim\n",
    "        convs.append(SAGEConv(in_dim, hid_dim))\n",
    "        # các layer hidden: hid_dim -> hid_dim\n",
    "        for _ in range(num_layers - 1):\n",
    "            convs.append(SAGEConv(hid_dim, hid_dim))\n",
    "        self.convs = nn.ModuleList(convs)\n",
    "\n",
    "        self.lin_out = nn.Linear(hid_dim, out_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        out = self.lin_out(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4a62742",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, weight=None, reduction=\"mean\"):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        logp = F.log_softmax(logits, dim=1)\n",
    "        p = logp.exp()\n",
    "\n",
    "        target = target.view(-1, 1)\n",
    "        logp_t = logp.gather(1, target).squeeze(1)\n",
    "        p_t    = p.gather(1, target).squeeze(1)\n",
    "\n",
    "        focal_term = (1 - p_t) ** self.gamma\n",
    "        loss = - focal_term * logp_t\n",
    "\n",
    "        if self.weight is not None:\n",
    "            w = self.weight[target.squeeze(1)].view(-1)\n",
    "            loss = loss * w\n",
    "\n",
    "        if self.reduction == \"mean\":\n",
    "            return loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss\n",
    "\n",
    "\n",
    "def train_one_config(config, loss_type=\"ce\", class_weights=None,\n",
    "                     max_epochs=400, patience=30, verbose=False):\n",
    "    in_dim  = train_data.x.size(1)\n",
    "    out_dim = 2\n",
    "\n",
    "    model = GraphSAGE(\n",
    "        in_dim=in_dim,\n",
    "        hid_dim=config[\"hid_dim\"],\n",
    "        out_dim=out_dim,\n",
    "        num_layers=config[\"num_layers\"],\n",
    "        dropout=config.get(\"dropout\", 0.5),\n",
    "    ).to(device)\n",
    "\n",
    "    cw = class_weights.to(device) if class_weights is not None else None\n",
    "\n",
    "    if loss_type == \"ce\":\n",
    "        crit = nn.CrossEntropyLoss(weight=cw)\n",
    "    elif loss_type == \"focal\":\n",
    "        crit = FocalLoss(gamma=config.get(\"gamma\", 2.0), weight=cw)\n",
    "    else:\n",
    "        raise ValueError(\"loss_type must be 'ce' or 'focal'\")\n",
    "\n",
    "    opt = torch.optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=config[\"lr\"],\n",
    "        weight_decay=config[\"weight_decay\"],\n",
    "    )\n",
    "\n",
    "    def eval_for_search(data):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            out = model(data.x.to(device), data.edge_index.to(device))\n",
    "            loss = crit(out, data.y.to(device)).item()\n",
    "            preds = out.argmax(dim=1).cpu().numpy()\n",
    "            y_true = data.y.cpu().numpy()\n",
    "            macro_f1 = f1_score(y_true, preds, average=\"macro\", zero_division=0)\n",
    "        return loss, macro_f1\n",
    "\n",
    "    best_state = None\n",
    "    best_val_macro = -1.0\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        model.train()\n",
    "        opt.zero_grad()\n",
    "        out = model(train_data.x.to(device), train_data.edge_index.to(device))\n",
    "        loss_train = crit(out, train_data.y.to(device))\n",
    "        loss_train.backward()\n",
    "        opt.step()\n",
    "\n",
    "        val_loss, val_macro = eval_for_search(val_data)\n",
    "\n",
    "        if verbose and (epoch % 20 == 0 or epoch == 1):\n",
    "            print(f\"[{config.get('name','?')}] Epoch {epoch:03d} \"\n",
    "                  f\"- train_loss={loss_train.item():.4f} \"\n",
    "                  f\"- val_loss={val_loss:.4f} \"\n",
    "                  f\"- val_macro={val_macro:.4f}\")\n",
    "\n",
    "        if val_macro > best_val_macro + 1e-4:\n",
    "            best_val_macro = val_macro\n",
    "            best_state = torch.save(model.state_dict(), \"tmp_best_model.pt\")\n",
    "            best_state = torch.load(\"tmp_best_model.pt\", map_location=\"cpu\")\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            if verbose:\n",
    "                print(f\"Early stop (no improve {patience} epochs)\")\n",
    "            break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    return model, best_val_macro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7292a79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9. Hàm tune threshold + evaluate_with_threshold\n",
    "# ============================================================\n",
    "\n",
    "@torch.no_grad()\n",
    "def tune_threshold_on_val(model, data, thresholds=None):\n",
    "    model.eval()\n",
    "    if thresholds is None:\n",
    "        thresholds = np.linspace(0.1, 0.9, 17)\n",
    "\n",
    "    out = model(data.x.to(device), data.edge_index.to(device))\n",
    "    probs = F.softmax(out, dim=1)[:, 1].cpu().numpy()\n",
    "    y_true = data.y.cpu().numpy()\n",
    "\n",
    "    results = []\n",
    "    for t in thresholds:\n",
    "        preds = (probs >= t).astype(int)\n",
    "        f1_scam = f1_score(y_true, preds, pos_label=1, zero_division=0)\n",
    "        macro_f1 = f1_score(y_true, preds, average=\"macro\", zero_division=0)\n",
    "        results.append((t, f1_scam, macro_f1))\n",
    "\n",
    "    best = max(results, key=lambda x: x[2])  # theo macro-F1\n",
    "    best_t, best_f1_scam, best_macro = best\n",
    "\n",
    "    print(\"\\n=== Threshold search on VAL ===\")\n",
    "    for t, f1_s, f1_m in results:\n",
    "        print(f\"th={t:.2f}  F1_scam={f1_s:.4f}  macro_F1={f1_m:.4f}\")\n",
    "\n",
    "    print(f\"\\n>>> Best threshold = {best_t:.2f} \"\n",
    "          f\"(F1_scam={best_f1_scam:.4f}, macro_F1={best_macro:.4f})\")\n",
    "\n",
    "    return best_t, results\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_with_threshold(model, data, threshold, name=\"TEST\"):\n",
    "    model.eval()\n",
    "    out = model(data.x.to(device), data.edge_index.to(device))\n",
    "    probs = F.softmax(out, dim=1)[:, 1].cpu().numpy()\n",
    "    y_true = data.y.cpu().numpy()\n",
    "    preds = (probs >= threshold).astype(int)\n",
    "\n",
    "    f1_scam  = f1_score(y_true, preds, pos_label=1, zero_division=0)\n",
    "    micro_f1 = f1_score(y_true, preds, average=\"micro\", zero_division=0)\n",
    "    macro_f1 = f1_score(y_true, preds, average=\"macro\", zero_division=0)\n",
    "    cm = confusion_matrix(y_true, preds, labels=[0, 1])\n",
    "\n",
    "    print(f\"\\n{name} with threshold={threshold:.2f}\")\n",
    "    print(\"F1 (scam):\", f1_scam)\n",
    "    print(\"Micro-F1:\", micro_f1)\n",
    "    print(\"Macro-F1:\", macro_f1)\n",
    "    print(\"Confusion matrix:\\n\", cm)\n",
    "    print(\"\\nClassification report:\")\n",
    "    print(classification_report(y_true, preds, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1b7277c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training config: arch1_CE_no_weight (loss=ce) ===\n",
      "Config arch1_CE_no_weight val_macro = 0.4444\n",
      "\n",
      "=== Training config: arch1_CE_inv_freq (loss=ce) ===\n",
      "Config arch1_CE_inv_freq val_macro = 0.8062\n",
      "\n",
      "=== Training config: arch1_Focal_gamma1.5 (loss=focal) ===\n",
      "Config arch1_Focal_gamma1.5 val_macro = 0.8071\n",
      "\n",
      "=== Training config: arch1_Focal_gamma2.0 (loss=focal) ===\n",
      "Config arch1_Focal_gamma2.0 val_macro = 0.6712\n",
      "\n",
      "=== Training config: arch2_CE_no_weight (loss=ce) ===\n",
      "Config arch2_CE_no_weight val_macro = 0.4444\n",
      "\n",
      "=== Training config: arch2_CE_inv_freq (loss=ce) ===\n",
      "Config arch2_CE_inv_freq val_macro = 0.5663\n",
      "\n",
      "=== Training config: arch2_Focal_gamma1.5 (loss=focal) ===\n",
      "Config arch2_Focal_gamma1.5 val_macro = 0.5663\n",
      "\n",
      "=== Training config: arch2_Focal_gamma2.0 (loss=focal) ===\n",
      "Config arch2_Focal_gamma2.0 val_macro = 0.5663\n",
      "\n",
      "=== Training config: arch3_CE_no_weight (loss=ce) ===\n",
      "Config arch3_CE_no_weight val_macro = 0.4444\n",
      "\n",
      "=== Training config: arch3_CE_inv_freq (loss=ce) ===\n",
      "Config arch3_CE_inv_freq val_macro = 0.5663\n",
      "\n",
      "=== Training config: arch3_Focal_gamma1.5 (loss=focal) ===\n",
      "Config arch3_Focal_gamma1.5 val_macro = 0.5663\n",
      "\n",
      "=== Training config: arch3_Focal_gamma2.0 (loss=focal) ===\n",
      "Config arch3_Focal_gamma2.0 val_macro = 0.5663\n",
      "\n",
      "=== Training config: arch4_CE_no_weight (loss=ce) ===\n",
      "Config arch4_CE_no_weight val_macro = 0.4444\n",
      "\n",
      "=== Training config: arch4_CE_inv_freq (loss=ce) ===\n",
      "Config arch4_CE_inv_freq val_macro = 0.5663\n",
      "\n",
      "=== Training config: arch4_Focal_gamma1.5 (loss=focal) ===\n",
      "Config arch4_Focal_gamma1.5 val_macro = 0.5663\n",
      "\n",
      "=== Training config: arch4_Focal_gamma2.0 (loss=focal) ===\n",
      "Config arch4_Focal_gamma2.0 val_macro = 0.5663\n",
      "\n",
      ">>> BEST CONFIG OVERALL\n",
      "Best arch: {'name': 'arch1', 'hid_dim': 64, 'num_layers': 8, 'dropout': 0.5, 'lr': 0.01, 'weight_decay': 0.0005, 'alpha': 0.1}\n",
      "Best loss config: {'name': 'Focal_gamma1.5', 'loss_type': 'focal', 'class_weights': tensor([1.0000, 4.0004]), 'gamma': 1.5}\n",
      "Best val macro-F1: 0.8071125475058363\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 10. Grid search arch × loss, chọn best model\n",
    "# ============================================================\n",
    "\n",
    "arch_space = [\n",
    "    {\"name\": \"arch1\", \"hid_dim\": 64,  \"num_layers\": 8,  \"dropout\": 0.5,\n",
    "     \"lr\": 1e-2, \"weight_decay\": 5e-4, \"alpha\": 0.1},\n",
    "    {\"name\": \"arch2\", \"hid_dim\": 64,  \"num_layers\": 16, \"dropout\": 0.5,\n",
    "     \"lr\": 1e-2, \"weight_decay\": 5e-4, \"alpha\": 0.1},\n",
    "    {\"name\": \"arch3\", \"hid_dim\": 128, \"num_layers\": 16, \"dropout\": 0.5,\n",
    "     \"lr\": 5e-3, \"weight_decay\": 5e-4, \"alpha\": 0.1},\n",
    "    {\"name\": \"arch4\", \"hid_dim\": 128, \"num_layers\": 32, \"dropout\": 0.5,\n",
    "     \"lr\": 5e-3, \"weight_decay\": 1e-3,\"alpha\": 0.1},\n",
    "]\n",
    "\n",
    "loss_configs = [\n",
    "    {\"name\": \"CE_no_weight\",   \"loss_type\": \"ce\",\n",
    "     \"class_weights\": torch.tensor([1.0, 1.0])},\n",
    "    {\"name\": \"CE_inv_freq\",    \"loss_type\": \"ce\",\n",
    "     \"class_weights\": norm_inv_freq},\n",
    "    {\"name\": \"Focal_gamma1.5\", \"loss_type\": \"focal\",\n",
    "     \"class_weights\": norm_inv_freq, \"gamma\": 1.5},\n",
    "    {\"name\": \"Focal_gamma2.0\", \"loss_type\": \"focal\",\n",
    "     \"class_weights\": norm_inv_freq, \"gamma\": 2.0},\n",
    "]\n",
    "\n",
    "best_model = None\n",
    "best_arch  = None\n",
    "best_loss_cfg = None\n",
    "best_val_macro = -1.0\n",
    "\n",
    "for arch in arch_space:\n",
    "    for lc in loss_configs:\n",
    "        cfg = arch.copy()\n",
    "        cfg[\"name\"] = arch[\"name\"] + \"_\" + lc[\"name\"]\n",
    "        if lc[\"loss_type\"] == \"focal\":\n",
    "            cfg[\"gamma\"] = lc[\"gamma\"]\n",
    "\n",
    "        print(f\"\\n=== Training config: {cfg['name']} (loss={lc['loss_type']}) ===\")\n",
    "        model_cfg, val_macro = train_one_config(\n",
    "            cfg,\n",
    "            loss_type=lc[\"loss_type\"],\n",
    "            class_weights=lc[\"class_weights\"],\n",
    "            max_epochs=300,\n",
    "            patience=40,\n",
    "            verbose=False,\n",
    "        )\n",
    "        print(f\"Config {cfg['name']} val_macro = {val_macro:.4f}\")\n",
    "\n",
    "        if val_macro > best_val_macro:\n",
    "            best_val_macro = val_macro\n",
    "            best_model = model_cfg\n",
    "            best_arch = arch\n",
    "            best_loss_cfg = lc\n",
    "\n",
    "print(\"\\n>>> BEST CONFIG OVERALL\")\n",
    "print(\"Best arch:\", best_arch)\n",
    "print(\"Best loss config:\", best_loss_cfg)\n",
    "print(\"Best val macro-F1:\", best_val_macro)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "907eeabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Threshold search on VAL ===\n",
      "th=0.10  F1_scam=0.3433  macro_F1=0.2173\n",
      "th=0.15  F1_scam=0.3543  macro_F1=0.2754\n",
      "th=0.20  F1_scam=0.3586  macro_F1=0.2966\n",
      "th=0.25  F1_scam=0.3607  macro_F1=0.3037\n",
      "th=0.30  F1_scam=0.6200  macro_F1=0.7608\n",
      "th=0.35  F1_scam=0.6526  macro_F1=0.7859\n",
      "th=0.40  F1_scam=0.6804  macro_F1=0.8063\n",
      "th=0.45  F1_scam=0.6758  macro_F1=0.8047\n",
      "th=0.50  F1_scam=0.6781  macro_F1=0.8071\n",
      "th=0.55  F1_scam=0.5862  macro_F1=0.7560\n",
      "th=0.60  F1_scam=0.4032  macro_F1=0.6572\n",
      "th=0.65  F1_scam=0.3819  macro_F1=0.6463\n",
      "th=0.70  F1_scam=0.3677  macro_F1=0.6391\n",
      "th=0.75  F1_scam=0.3538  macro_F1=0.6319\n",
      "th=0.80  F1_scam=0.3384  macro_F1=0.6237\n",
      "th=0.85  F1_scam=0.3272  macro_F1=0.6178\n",
      "th=0.90  F1_scam=0.3127  macro_F1=0.6100\n",
      "\n",
      ">>> Best threshold = 0.50 (F1_scam=0.6781, macro_F1=0.8071)\n",
      "\n",
      "TRAIN with threshold=0.50\n",
      "F1 (scam): 0.8303492944714319\n",
      "Micro-F1: 0.9264747162850383\n",
      "Macro-F1: 0.891708250194773\n",
      "Confusion matrix:\n",
      " [[37233  2667]\n",
      " [ 1000  8974]]\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9738    0.9332    0.9531     39900\n",
      "           1     0.7709    0.8997    0.8303      9974\n",
      "\n",
      "    accuracy                         0.9265     49874\n",
      "   macro avg     0.8724    0.9164    0.8917     49874\n",
      "weighted avg     0.9333    0.9265    0.9285     49874\n",
      "\n",
      "\n",
      "VAL with threshold=0.50\n",
      "F1 (scam): 0.6781479390175043\n",
      "Micro-F1: 0.8933383233532934\n",
      "Macro-F1: 0.8071125475058363\n",
      "Confusion matrix:\n",
      " [[8347  203]\n",
      " [ 937 1201]]\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8991    0.9763    0.9361      8550\n",
      "           1     0.8554    0.5617    0.6781      2138\n",
      "\n",
      "    accuracy                         0.8933     10688\n",
      "   macro avg     0.8772    0.7690    0.8071     10688\n",
      "weighted avg     0.8903    0.8933    0.8845     10688\n",
      "\n",
      "\n",
      "TEST with threshold=0.50\n",
      "F1 (scam): 0.6944833380005601\n",
      "Micro-F1: 0.8979229041916168\n",
      "Macro-F1: 0.8166042076130293\n",
      "Confusion matrix:\n",
      " [[8357  193]\n",
      " [ 898 1240]]\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9030    0.9774    0.9387      8550\n",
      "           1     0.8653    0.5800    0.6945      2138\n",
      "\n",
      "    accuracy                         0.8979     10688\n",
      "   macro avg     0.8841    0.7787    0.8166     10688\n",
      "weighted avg     0.8954    0.8979    0.8899     10688\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 11. Tuning threshold & Evaluate trên train/val/test\n",
    "# ============================================================\n",
    "\n",
    "best_threshold, _ = tune_threshold_on_val(best_model, val_data)\n",
    "\n",
    "evaluate_with_threshold(best_model, train_data, best_threshold, name=\"TRAIN\")\n",
    "evaluate_with_threshold(best_model, val_data,   best_threshold, name=\"VAL\")\n",
    "evaluate_with_threshold(best_model, test_data,  best_threshold, name=\"TEST\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
